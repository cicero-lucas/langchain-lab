{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b468c3",
   "metadata": {},
   "source": [
    "# üìö LangChain com RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "O **RAG** combina **busca de informa√ß√µes** com **modelos de linguagem** para gerar respostas mais precisas e atualizadas. Ele √© ideal para:\n",
    "\n",
    "- üîç Responder perguntas com base em documentos\n",
    "- üß† Sistemas de QA (Pergunta e Resposta)\n",
    "- üóÇÔ∏è Chatbots com base em conhecimento espec√≠fico\n",
    "\n",
    "---\n",
    "\n",
    "### O RAG (Retrieval-Augmented Generation) √© uma evolu√ß√£o moderna das abordagens que voc√™ j√° usou anteriormente com LangChain, como:\n",
    "\n",
    "Chains (LLMChain, SequentialChain, RouterChain)\n",
    "Mem√≥rias (ConversationBufferMemory, SummaryMemory, etc.)\n",
    "RouterChain para direcionar fluxos\n",
    "Prompt personalizado para controle de sa√≠da\n",
    "üß† O que torna o RAG mais moderno?\n",
    "RAG integra busca de conhecimento externo (como documentos, bases de dados, APIs) diretamente no fluxo de gera√ß√£o de texto. Isso permite:\n",
    "\n",
    "Tradicional LangChain\tRAG com LangChain\n",
    "Usa apenas o LLM e mem√≥ria\tUsa LLM + busca inteligente\n",
    "Depende do contexto interno\tConsulta fontes externas\n",
    "Bom para conversas\tExcelente para perguntas baseadas em conhecimento\n",
    "Fluxo fixo\tFluxo din√¢mico e adapt√°vel\n",
    "\n",
    "## üß± Componentes do RAG\n",
    "\n",
    "1. **Documentos**: Fonte de conhecimento (PDFs, textos, bases de dados).\n",
    "2. **Embeddings**: Representa√ß√£o vetorial dos documentos.\n",
    "3. **Vector Store**: Armazena os embeddings (ex: FAISS).\n",
    "4. **Retriever**: Recupera documentos relevantes.\n",
    "5. **LLM**: Gera a resposta com base nos documentos recuperados.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Exemplo com Gemini\n",
    "\n",
    "```bash\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1. Carrega os documentos\n",
    "loader = TextLoader(\"meus_documentos.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Cria os embeddings com Gemini\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 3. Cria o vetor de busca (FAISS)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 4. Cria o retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 5. Inicializa o modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# 6. Cria a cadeia RAG (Retrieval + QA)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 7. Faz uma pergunta\n",
    "query = \"Quais s√£o os principais benef√≠cios da energia solar?\"\n",
    "result = rag_chain({\"query\": query})\n",
    "\n",
    "# Exibe a resposta\n",
    "print(result[\"result\"])\n",
    "```\n",
    "\n",
    "### 2. üß† Conversational RAG (ConversationalRetrievalChain)\n",
    "Mant√©m o contexto da conversa enquanto busca informa√ß√µes.\n",
    "\n",
    "```bash\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Mem√≥ria para manter o hist√≥rico\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Cadeia conversacional com busca\n",
    "conversational_rag = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Consulta com hist√≥rico\n",
    "response = conversational_rag.run({\"question\": \"E quais s√£o os desafios?\", \"chat_history\": []})\n",
    "\n",
    "\n",
    "```\n",
    "### 3. üß™ Custom RAG com Prompt Personalizado\n",
    "Permite usar um prompt espec√≠fico para controlar como os documentos s√£o usados.\n",
    "\n",
    "```bash\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Prompt customizado\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Use o contexto abaixo para responder √† pergunta.\n",
    "    Contexto: {context}\n",
    "    Pergunta: {question}\n",
    "    Resposta:\"\"\"\n",
    ")\n",
    "\n",
    "# Cadeia com prompt customizado\n",
    "custom_rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "response = custom_rag.run(\"Como funciona a energia solar?\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4. üß± Multi-Document RAG (MapReduce ou Refine)\n",
    "Divide documentos longos em partes e processa em etapas.\n",
    "```bash\n",
    "# MapReduce: processa partes e depois combina\n",
    "map_reduce_rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "\n",
    "# Refine: gera uma resposta inicial e refina com mais contexto\n",
    "refine_rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 5. üß≠ RAG com RouterChain\n",
    "Roteia perguntas para diferentes bases de conhecimento ou cadeias.\n",
    "\n",
    "```bash\n",
    "from langchain.chains.router import MultiRetrievalQAChain\n",
    "\n",
    "# Dois retrievers diferentes\n",
    "retriever_energia = retriever  # Base de energia\n",
    "retriever_saude = FAISS.from_documents(outros_docs, embeddings).as_retriever()\n",
    "\n",
    "# Mapeamento de t√≥picos\n",
    "retrievers = {\n",
    "    \"energia\": retriever_energia,\n",
    "    \"saude\": retriever_saude\n",
    "}\n",
    "\n",
    "# Cadeia com roteamento\n",
    "router_rag = MultiRetrievalQAChain.from_retrievers(llm=llm, retriever_names_to_retrievers=retrievers)\n",
    "\n",
    "response = router_rag.run(\"Quais s√£o os efeitos da energia solar na sa√∫de?\")\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09da4ff",
   "metadata": {},
   "source": [
    "# üìÑ LangChain - DocumentLoader\n",
    "\n",
    "O `DocumentLoader` √© respons√°vel por **carregar documentos** de diversas fontes e formatos para serem processados por embeddings, vetores e LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tipos Comuns de DocumentLoader\n",
    "\n",
    "### 1. üìÉ TextLoader\n",
    "Carrega arquivos `.txt`.\n",
    "\n",
    "```bash\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"documento.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "arquivo=\"../docs/apostila.txt\"\n",
    "\n",
    "loader=TextLoader(arquivo)\n",
    "\n",
    "document=loader.load()\n",
    "\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf56c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3210c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "Chat=ChatOpenAI()\n",
    "\n",
    "chain=load_qa_chain(llm=Chat, chain_type=\"stuff\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta=\"o que √© esse documneto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5a9c2",
   "metadata": {},
   "source": [
    "Carregamento de Videos do Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# Carrega a transcri√ß√£o do v√≠deo\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/shorts/J9ss7uMYKlQ\",\n",
    "    add_video_info=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Agora voc√™ pode usar os documentos com embeddings e RAG\n",
    "print(documents[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Embeddings e vetor\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Cadeia RAG\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# Pergunta sobre o v√≠deo\n",
    "response = rag_chain.run(\"Quais s√£o os principais t√≥picos abordados no v√≠deo?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26b52b",
   "metadata": {},
   "source": [
    "Texto Splites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1300b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import string\n",
    "\n",
    "texto=\"Desenvolvedor com foco em IA Generativa e Data & AI, aplicando solu√ß√µes inteligentes baseadas em modelos de linguagem (LLMs), NLP e arquitetura em nuvem. Atua no desenvolvimento de APIs com Python (FastAPI, Flask) e Node.js, integrando LLMs com LangChain, Amazon Bedrock, Amazon Lex e SageMaker. Trabalha com bancos relacionais (MySQL, RDS) e NoSQL (MongoDB, DynamoDB), al√©m de pipelines de dados com Glue, Lambda e S3. Possui experi√™ncia com Docker, EC2, API Gateway e arquitetura serverless. No front-end, utiliza React e TypeScript para interfaces ricas em experi√™ncia. Participa de squads √°geis com Scrum, colaborando em solu√ß√µes de ponta a ponta. Certificado AWS Cloud Practitioner e GenAI Technical. Busca consolidar-se como desenvolvedor especialista em IA e Dados, entregando valor com solu√ß√µes robustas, escal√°veis e baseadas em cloud.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546abcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSize = 50\n",
    "chunkOverlap = 0\n",
    "\n",
    "char_split = CharacterTextSplitter(\n",
    "chunk_size=chunkSize,\n",
    "chunk_overlap=chunkOverlap,\n",
    "separator= \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texto=\"\".join(f\"{string.ascii_lowercase}\" for _ in range(5))\n",
    "\n",
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = char_split.split_text(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22166005",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSize = 50\n",
    "chunkOverlap = 0\n",
    "\n",
    "char_split = CharacterTextSplitter(\n",
    "chunk_size=chunkSize,\n",
    "chunk_overlap=chunkOverlap,\n",
    "separator= \"\"\n",
    ")\n",
    "\n",
    "split = char_split.split_text(texto)\n",
    "\n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01930ae2",
   "metadata": {},
   "source": [
    "### Vector _ storages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ed65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file= \"../docs/apostila.txt\"\n",
    "loader= TextLoader(file)\n",
    "pages=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_split=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"/n/n\",\"/n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4839313",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=recur_split.split_documents(pages)\n",
    "for i in document:\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma  # Corrigido de \"Choroma\" para \"Chroma\"\n",
    "\n",
    "# Corrigido nome da vari√°vel\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "diretorio = \"../files/chroma_vectostores\"  # Corrigido \"choma_vectostores\"\n",
    "\n",
    "# Corrigido nome da classe e argumentos da fun√ß√£o\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document,  # Corrigido \"document\" para \"documents\"\n",
    "    embedding=embedding_model,  # Corrigido \"embeddings, embeddinf_model\"\n",
    "    persist_directory=diretorio  # Corrigido \"persist_direco\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.collection.cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822abcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializa o vector store\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=diretorio\n",
    ")\n",
    "\n",
    "# Consulta\n",
    "pergunta = \"principais manipula√ß√£o de strings\"  # Corrigido \"princiapis manipil√ß√£o\"\n",
    "\n",
    "# Busca por similaridade\n",
    "docs = vector_store.similarity_search(pergunta, k=5)\n",
    "\n",
    "# Exibe os resultados\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"=== {doc.metadata}\\n\")  # Corrigido \"/n\" para \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc307a",
   "metadata": {},
   "source": [
    "\n",
    "## üìå O que √© um Retriever?\n",
    "Um retriever √© uma interface que, dado um texto de entrada (como uma pergunta), retorna os documentos mais relevantes de um reposit√≥rio (como um VectorStore). Ele √© usado principalmente em sistemas de RAG (Retrieval-Augmented Generation), onde a IA busca informa√ß√µes antes de gerar uma resposta.\n",
    "\n",
    "‚úÖ Exemplo b√°sico com Chroma e LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f73362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Cria√ß√£o do modelo de embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Carregamento do vector store\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"../files/chroma_vectostores\"\n",
    ")\n",
    "\n",
    "# Cria√ß√£o do retriever a partir do vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Consulta\n",
    "query = \"principais fun√ß√µes de manipula√ß√£o de strings\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Exibi√ß√£o dos resultados\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"=== {doc.metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f735a4",
   "metadata": {},
   "source": [
    "A busca sem√¢ntica √© uma t√©cnica de recupera√ß√£o de informa√ß√µes que entende o significado das palavras em vez de apenas procurar por correspond√™ncia exata de termos. Isso √© especialmente √∫til quando queremos encontrar documentos ou respostas que s√£o conceitualmente relevantes, mesmo que n√£o usem exatamente as mesmas palavras da consulta.\n",
    "\n",
    "üß† Diferen√ßa entre busca tradicional e busca sem√¢ntica:\n",
    "Tipo de Busca\tComo funciona\tExemplo\n",
    "Tradicional\tProcura palavras exatas no texto\tConsulta: \"manipula√ß√£o de strings\" s√≥ retorna documentos com essas palavras\n",
    "Sem√¢ntica\tUsa embeddings para entender o significado\tConsulta: \"como trabalhar com texto\" pode retornar documentos sobre manipula√ß√£o de strings\n",
    "üîß Como funciona na pr√°tica com LangChain:\n",
    "Documentos s√£o convertidos em vetores usando um modelo de embeddings (como OpenAIEmbeddings).\n",
    "A consulta do usu√°rio tamb√©m √© convertida em vetor.\n",
    "O sistema compara os vetores usando similaridade de cosseno ou outra m√©trica.\n",
    "Retorna os documentos mais sem√¢nticamente pr√≥ximos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Modelo de embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Carrega o vetor store\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"../files/chroma_vectostores\"\n",
    ")\n",
    "\n",
    "# Busca sem√¢ntica\n",
    "query = \"como trabalhar com texto em Python\"\n",
    "docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# Exibe os resultados\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"=== {doc.metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ddaa9",
   "metadata": {},
   "source": [
    "üîç max_relevance ‚Äî O que √©?\n",
    "Em alguns retrievers (como os do LangChain), voc√™ pode configurar o modo de busca para priorizar relev√¢ncia m√°xima. Isso significa que o sistema ser√° mais rigoroso ao escolher os documentos mais pr√≥ximos semanticamente da sua consulta.\n",
    "\n",
    "‚úÖ Como usar no LangChain\n",
    "Se estiver usando um retriever criado com .as_retriever(), voc√™ pode configurar o modo assim:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8145e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"lambda_mult\": 0.8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43123799",
   "metadata": {},
   "source": [
    "A filtragem na busca sem√¢ntica √© o processo de limitar ou refinar os resultados com base em crit√©rios espec√≠ficos, como metadados, tags, datas, categorias, etc. Isso √© √∫til quando voc√™ quer buscar documentos relevantes dentro de um contexto espec√≠fico.\n",
    "\n",
    "üîç Exemplo de filtragem com Chroma e LangChain\n",
    "Suponha que seus documentos tenham metadados como \"categoria\": \"Python\" ou \"autor\": \"Cicero\".\n",
    "\n",
    "Voc√™ pode aplicar filtros assim:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e82e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\"categoria\": \"Python\"}  # Filtra por metadado\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"como manipular strings\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"=== {doc.metadata}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv LangChain)",
   "language": "python",
   "name": "meu_ambiente_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
