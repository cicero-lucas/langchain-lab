{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3853934a",
   "metadata": {},
   "source": [
    "# üîó LangChain - Conceito de Chains com Gemini\n",
    "\n",
    "As **Chains** em LangChain s√£o fluxos de execu√ß√£o que conectam modelos de linguagem com outras ferramentas, mem√≥rias e l√≥gicas. Elas permitem criar aplica√ß√µes mais estruturadas e reutiliz√°veis.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tipos de Chains com Exemplos Completos\n",
    "\n",
    "### 1. üí¨ ConversationChain\n",
    "Cadeia simples que conecta um modelo de linguagem com uma mem√≥ria para manter o contexto da conversa.\n",
    "\n",
    "```bash\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Inicializa o modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Cria a mem√≥ria para armazenar o hist√≥rico da conversa\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Cria a cadeia de conversa\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Executa uma intera√ß√£o\n",
    "response = conversation.run(\"Qual √© a capital do Brasil?\")\n",
    "```\n",
    "### 2. üìÑ LLMChain\n",
    "Executa uma tarefa com base em um prompt personalizado. Ideal para tarefas espec√≠ficas como gera√ß√£o de texto, resumo, tradu√ß√£o, etc.\n",
    "```bash\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Inicializa o modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Define o prompt com vari√°veis\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tema\"],\n",
    "    template=\"Explique de forma simples o seguinte tema: {tema}\"\n",
    ")\n",
    "\n",
    "# Cria a cadeia com o prompt e o modelo\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Executa a cadeia com um tema\n",
    "response = chain.run(\"Blockchain\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 3. üß† SimpleSequentialChain\n",
    "Executa m√∫ltiplas cadeias em sequ√™ncia, passando a sa√≠da de uma como entrada da pr√≥xima.\n",
    "```bash\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Primeira cadeia: gera uma ideia\n",
    "prompt_1 = PromptTemplate(input_variables=[\"tema\"], template=\"Crie um t√≠tulo criativo sobre {tema}\")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "# Segunda cadeia: desenvolve um par√°grafo com base no t√≠tulo\n",
    "prompt_2 = PromptTemplate(input_variables=[\"titulo\"], template=\"Escreva um par√°grafo sobre: {titulo}\")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "# Cadeia sequencial\n",
    "sequential_chain = SimpleSequentialChain(chains=[chain_1, chain_2])\n",
    "\n",
    "# Executa a cadeia\n",
    "response = sequential_chain.run(\"intelig√™ncia artificial na educa√ß√£o\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 4. üîÑ SequentialChain (com m√∫ltiplas vari√°veis)\n",
    "Permite controle mais detalhado sobre entradas e sa√≠das entre as etapas.\n",
    "```bash\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Cadeia 1: gera um t√≠tulo\n",
    "prompt_1 = PromptTemplate(input_variables=[\"tema\"], template=\"Crie um t√≠tulo para um artigo sobre {tema}\")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"titulo\")\n",
    "\n",
    "# Cadeia 2: gera um resumo com base no t√≠tulo\n",
    "prompt_2 = PromptTemplate(input_variables=[\"titulo\"], template=\"Escreva um resumo para o artigo: {titulo}\")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"resumo\")\n",
    "\n",
    "# Cadeia sequencial com m√∫ltiplas vari√°veis\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2],\n",
    "    input_variables=[\"tema\"],\n",
    "    output_variables=[\"titulo\", \"resumo\"]\n",
    ")\n",
    "\n",
    "# Executa a cadeia\n",
    "result = overall_chain.run({\"tema\": \"tecnologia sustent√°vel\"})\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 5. üß™ TransformChain (avan√ßado)\n",
    "Aplica transforma√ß√µes personalizadas nos dados entre etapas. Ideal para pipelines complexos.\n",
    "\n",
    "```bash\n",
    "from langchain.chains import TransformChain\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "\n",
    "# Fun√ß√£o de transforma√ß√£o personalizada\n",
    "def transformar(inputs):\n",
    "    texto = inputs[\"texto\"]\n",
    "    return {\"texto_maiusculo\": texto.upper()}\n",
    "\n",
    "# Cria a cadeia de transforma√ß√£o\n",
    "chain = TransformChain(\n",
    "    input_variables=[\"texto\"],\n",
    "    output_variables=[\"texto_maiusculo\"],\n",
    "    transform=transformar\n",
    ")\n",
    "\n",
    "# Executa a transforma√ß√£o\n",
    "output = chain.run({\"texto\": \"langchain √© poderoso\"})\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# üö¶ LangChain - RouterChain com Gemini\n",
    "\n",
    "A **RouterChain** permite **direcionar dinamicamente** uma entrada para diferentes cadeias (chains), com base em regras, inten√ß√£o do usu√°rio ou classifica√ß√£o. √â ideal para:\n",
    "\n",
    "- üß† Assistentes multifuncionais\n",
    "- üõ†Ô∏è Aplica√ß√µes com m√∫ltiplos fluxos\n",
    "- üóÇÔ∏è Classifica√ß√£o de tarefas\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Como Funciona\n",
    "\n",
    "1. O usu√°rio envia uma entrada.\n",
    "2. Um modelo de linguagem (LLM) analisa a inten√ß√£o.\n",
    "3. A entrada √© roteada para a cadeia apropriada.\n",
    "4. A resposta √© gerada com base na cadeia selecionada.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Exemplo com Gemini\n",
    "\n",
    "```bash\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "\n",
    "# Inicializa o modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Define prompts espec√≠ficos para cada tipo de tarefa\n",
    "math_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Resolva o seguinte problema matem√°tico: {input}\"\n",
    ")\n",
    "\n",
    "code_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Escreva um c√≥digo Python para: {input}\"\n",
    ")\n",
    "\n",
    "general_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Responda de forma geral: {input}\"\n",
    ")\n",
    "\n",
    "# Cria cadeias espec√≠ficas\n",
    "math_chain = LLMChain(llm=llm, prompt=math_prompt)\n",
    "code_chain = LLMChain(llm=llm, prompt=code_prompt)\n",
    "general_chain = LLMChain(llm=llm, prompt=general_prompt)\n",
    "\n",
    "# Define os destinos com descri√ß√µes\n",
    "destination_chains = {\n",
    "    \"math\": math_chain,\n",
    "    \"code\": code_chain,\n",
    "    \"general\": general_chain\n",
    "}\n",
    "\n",
    "# Prompt para decidir o destino\n",
    "router_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Classifique a seguinte entrada como 'math', 'code' ou 'general': {input}\"\n",
    ")\n",
    "\n",
    "# Cadeia de roteamento\n",
    "router_chain = MultiPromptChain(\n",
    "    llm_chain=LLMChain(llm=llm, prompt=router_prompt),\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=general_chain\n",
    ")\n",
    "\n",
    "# Testa o roteador com uma entrada\n",
    "response = router_chain.run(\"Calcule a raiz quadrada de 144\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14027274",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fd8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ee7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-tubo\")\n",
    "memory=ConversationBufferMemory()\n",
    "chain=ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.predict(input(\"ol√°\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "propmt_templete=PromptTemplate(\"\"\"\n",
    "Essa √© uma conversa amigavel :\n",
    "conversa atual: \n",
    "{history}\n",
    "Human:{input}\n",
    "AI:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat= ChatOpenAI()\n",
    "memory= ConversationBufferMemory()\n",
    "chain=ConversationChain(\n",
    "    prompt=propmt_templete,\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce788b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(input(\"ola\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114cf10",
   "metadata": {},
   "source": [
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "chat=ChatOpenAI(model=\"gpt-3.5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e22454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "\n",
    "prompt= PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Escolha o melhor nome para mim sobre uma empresa que sesenvolve solu√ßoes em {produto}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain= LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "produto=\"llm com Rag\"\n",
    "chain.run(produto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae34af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv LangChain)",
   "language": "python",
   "name": "meu_ambiente_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
