{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ec7bca",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\",openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\" Conte uma historia sobre apredizado de m√°quina\"\n",
    "llm.invoke(prompt) #chama minha ia passando o promp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b026d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerando o texto parte a parte\n",
    "\n",
    "prompt=\" Conte uma historia sobre apredizado de m√°quina\"\n",
    "for trecho in llm.stream(prompt):\n",
    "    llm.invoke(trecho, end=\"\") #chama minha ia passando o promp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fazer perguntas em lote\n",
    "perguntas=[\n",
    "    \"o que √© Brasil?\",\n",
    "    \"o que √© brasileiros?\",\n",
    "    \"o que √© samba?\"\n",
    "]\n",
    "\n",
    "llm.abatch(perguntas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2053a79",
   "metadata": {},
   "source": [
    "## ChatModels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #chatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage #modelos de cofigura a messangem\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens=[\n",
    "    #SystemMessage define o comportamento do assistente\n",
    "    SystemMessage(content=\"Voc√™ √© um asssistente que responde com ir√¥nia e zueira\"),\n",
    "    HumanMessage(content=\"Qual o papel da mem√≥ria cache?\")\n",
    "]\n",
    "\n",
    "resposta= chat.invoke(mensagens)\n",
    "\n",
    "resposta #tente ver o resposta.contate e resposta.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31431f",
   "metadata": {},
   "source": [
    "## Prompt few shot\n",
    "\n",
    "Few-shot prompting √© uma t√©cnica usada com modelos de linguagem (como o ChatGPT) onde voc√™ d√° alguns exemplos de como o modelo deve responder antes de fazer sua pergunta real.\n",
    "\n",
    "A ideia √© ensinar o modelo pelo exemplo, sem precisar reprogramar nada.\n",
    "\n",
    "Tipo de Prompt\tExemplo\tCaracter√≠stica\n",
    "Zero-shot\t\"Traduza para ingl√™s: Ol√° mundo\"\tNenhum exemplo\n",
    "One-shot\t+1 exemplo antes da tarefa\tUm exemplo\n",
    "Few-shot\t+2 ou mais exemplos\tAlguns exemplos\n",
    "Fine-tuning\tTreinamento com muitos exemplos\tRequer ajuste do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat=ChatOpenAI(model=\"gpt.3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b17722",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens=[\n",
    "    SystemMessage(content=\"voce sera ironico\"),\n",
    "    HumanMessage(content=\"o que √© tv?\"),\n",
    "    AIMessage(content=\"televis√£o √© um dispositivo que assitimos\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "import langchain\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6efc78",
   "metadata": {},
   "source": [
    "## Cacheamento\n",
    "\n",
    " O cacheamento em LangChain √© um recurso que permite evitar chamadas repetidas √† API (como OpenAI ou Gemini) quando o mesmo prompt j√° foi processado antes. Isso ajuda a:\n",
    "\n",
    "‚úÖ Economizar tokens e custos\n",
    "‚úÖ Aumentar a velocidade de resposta\n",
    "‚úÖ Evitar chamadas desnecess√°rias √† API\n",
    "üîπ Como funciona o cache no LangChain?\n",
    "LangChain pode armazenar os resultados de chamadas LLM em cache usando diferentes backends, como:\n",
    "\n",
    "Em mem√≥ria (padr√£o, √∫til para testes r√°pidos)\n",
    "SQLite (persistente entre execu√ß√µes)\n",
    "Redis (para ambientes distribu√≠dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e961a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "#cacheamneto\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "chat= ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens=[\n",
    "    SystemMessage(content=\"Voc√™ √© um assistente irritante\"),\n",
    "    HumanMessage(content=\"Qual o primeiro dia da semana?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2131a",
   "metadata": {},
   "source": [
    "### Cacheamento em InmemoryCache \n",
    " um tipo de cache vol√°til (n√£o persistente) que guarda os resultados das chamadas LLM na mem√≥ria RAM enquanto o programa est√° rodando. Se voc√™ rodar o mesmo prompt duas vezes, ele retorna a resposta da primeira vez, sem chamar a API de novo.\n",
    "\n",
    " ¬¥¬¥¬¥¬¥ from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa80cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cacheamento em InmemoryCache \n",
    "# from langchain.cache import InMemoryCache\n",
    "#from langchain.globals import set_llm_cache \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "chat.invoke(mensagens)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae3649",
   "metadata": {},
   "source": [
    "### Cache em banco de dados\n",
    "posso te mostrar como usar cache persistente com SQLite ou at√© medir o tempo de resposta com e sem cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bcbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Carrega vari√°veis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Ativa o cache persistente com SQLite\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain_cache.db\"))\n",
    "\n",
    "# Inicializa o modelo\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define as mensagens\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Voc√™ √© um assistente que responde com ironia e zueira\"),\n",
    "    HumanMessage(content=\"Qual o papel da mem√≥ria cache?\")\n",
    "]\n",
    "\n",
    "# Primeira chamada (vai consultar a API e salvar no cache)\n",
    "start = time.time()\n",
    "resposta1 = chat.invoke(mensagens)\n",
    "print(\"Resposta 1:\", resposta1.content)\n",
    "print(\"Tempo 1:\", time.time() - start, \"segundos\")\n",
    "\n",
    "# Segunda chamada (vai usar o cache do SQLite)\n",
    "start = time.time()\n",
    "resposta2 = chat.invoke(mensagens)\n",
    "print(\"Resposta 2:\", resposta2.content)\n",
    "print(\"Tempo 2:\", time.time() - start, \"segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78089eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv LangChain)",
   "language": "python",
   "name": "meu_ambiente_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
